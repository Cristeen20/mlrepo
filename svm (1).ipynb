{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-title"
        ],
        "id": "WIVqku3OihAN"
      },
      "source": [
        "# Assignment 1-2: Support Vector Machine\n",
        "\n",
        "## Multiclass Support Vector Machine exercise\n",
        "\n",
        "\n",
        "In this exercise you will:\n",
        "    \n",
        "- implement a fully-vectorized **loss function** for the SVM\n",
        "- implement the fully-vectorized expression for its **analytic gradient**\n",
        "- **check your implementation** using numerical gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "4Wt_o63FihAP"
      },
      "source": [
        "## CIFAR-10 Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ8asDK0ihAQ",
        "outputId": "2ec2b294-25da-4f3b-f407-a889c542bbb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WrHhi-uMihAR"
      },
      "outputs": [],
      "source": [
        "from data_utils import load_pickle,load_CIFAR10,load_CIFAR_batch\n",
        "from download import _print_download_progress,maybe_download_and_extract\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kcb5vhhihAS",
        "outputId": "e1b15f40-a8d5-4fcc-84d7-47164387a4b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has apparently already been downloaded and unpacked.\n"
          ]
        }
      ],
      "source": [
        "# Load the raw CIFAR-10 data.\n",
        "url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "download_dir = \"./data\"\n",
        "maybe_download_and_extract(url,download_dir)\n",
        "\n",
        "# %%\n",
        "cifar10_dir = '/content/data/CIFAR-10/cifar-10-batches-py'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,y_train,X_test,y_test = load_CIFAR10(cifar10_dir)\n"
      ],
      "metadata": {
        "id": "MOpqhGB1vyey"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_QEVKUxxIfm",
        "outputId": "98920158-de5e-4ee1-b9e5-3b1ca1562a3c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape:  (50000, 32, 32, 3)\n",
            "Train labels shape:  (50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the number of samples for each set\n",
        "num_training = 49000\n",
        "num_validation = 1000\n",
        "num_test = 1000\n",
        "num_dev = 500"
      ],
      "metadata": {
        "id": "6ZBEV5CAv1ey"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our validation set will be num_validation points from the original training set.\n",
        "X_val = X_train[num_training:num_training + num_validation]\n",
        "y_val = y_train[num_training:num_training + num_validation]\n",
        "\n",
        "# Our training set will be the first num_train points from the original training set.\n",
        "X_train = X_train[:num_training]\n",
        "y_train = y_train[:num_training]\n",
        "\n",
        "# We will also make a development set, which is a small subset of the training set.\n",
        "mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "X_dev = X_train[mask]\n",
        "y_dev = y_train[mask]\n",
        "\n",
        "# We use the first num_test points of the original test set as our test set.\n",
        "X_test = X_test[:num_test]\n",
        "y_test = y_test[:num_test]\n",
        "\n",
        "# Print shapes to verify the splits\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Development data shape: ', X_dev.shape)\n",
        "print('Development labels shape: ', y_dev.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mvgioubowr5l",
        "outputId": "803cc5b0-a5ae-4714-b5bc-70834396b3c8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape:  (49000, 32, 32, 3)\n",
            "Train labels shape:  (49000,)\n",
            "Validation data shape:  (1000, 32, 32, 3)\n",
            "Validation labels shape:  (1000,)\n",
            "Development data shape:  (500, 32, 32, 3)\n",
            "Development labels shape:  (500,)\n",
            "Test data shape:  (1000, 32, 32, 3)\n",
            "Test labels shape:  (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing: reshape the image data into rows\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
        "\n",
        "# As a sanity check, print out the shapes of the data\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Development data shape: ', X_dev.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SGcLLAIw2U-",
        "outputId": "c09e9f05-8817-4798-b4af-9a8d3fd5615e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape:  (49000, 3072)\n",
            "Validation data shape:  (1000, 3072)\n",
            "Test data shape:  (1000, 3072)\n",
            "Development data shape:  (500, 3072)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing: subtract the mean image\n",
        "# first: compute the image mean based on the training data\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "print(mean_image[:10]) # print a few of the elements\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image\n",
        "plt.show()\n",
        "\n",
        "# second: subtract the mean image from train and test data\n",
        "X_train -= mean_image\n",
        "X_val -= mean_image\n",
        "X_test -= mean_image\n",
        "X_dev -= mean_image\n",
        "\n",
        "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
        "# only has to worry about optimizing a single weight matrix W.\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "_5FRgIRKxTju",
        "outputId": "aa008914-68fe-405a-9fbb-e5d18ebbc435"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[130.64189796 135.98173469 132.47391837 130.05569388 135.34804082\n",
            " 131.75402041 130.96055102 136.14328571 132.47636735 131.48467347]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87JwgaUHihAT"
      },
      "source": [
        "## SVM Classifier\n",
        "\n",
        "You need to complete `svm_loss_naive` which uses for loops to evaluate the multiclass SVM loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "j6jWQLsfihAT"
      },
      "outputs": [],
      "source": [
        "from builtins import range\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "from past.builtins import xrange\n",
        "\n",
        "def svm_loss_naive(W, X, y, reg):\n",
        "\n",
        "    dW = np.zeros(W.shape)\n",
        "\n",
        "    num_classes = W.shape[1]\n",
        "    num_train = X.shape[0]\n",
        "    loss = 0.0\n",
        "\n",
        "    for i in range(num_train):\n",
        "        scores = X[i].dot(W)\n",
        "        correct_class_score = scores[y[i]]\n",
        "        for j in range(num_classes):\n",
        "            if j == y[i]:\n",
        "                continue\n",
        "            margin = scores[j] - correct_class_score + 1\n",
        "            if margin > 0:\n",
        "                loss += margin\n",
        "                dW[:, j] += X[i]\n",
        "                dW[:, y[i]] -= X[i]\n",
        "\n",
        "    loss /= num_train\n",
        "    loss += reg * np.sum(W * W)\n",
        "\n",
        "    dW /= num_train\n",
        "    dW += 2 * reg * W\n",
        "\n",
        "    return loss, dW\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JxAW0kmihAU",
        "outputId": "8fed086d-ace1-4fdb-82ab-60a3a8134c98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 9.288473\n"
          ]
        }
      ],
      "source": [
        "# generate a random SVM weight matrix of small numbers\n",
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "\n",
        "loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "print('loss: %f' % (loss, ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR5q3WGhihAU"
      },
      "source": [
        "The `grad` returned from the function above is right now all zero. Derive and implement the gradient for the SVM cost function and implement it inline inside the function `svm_loss_naive`. You will find it helpful to interleave your new code inside the existing function.\n",
        "\n",
        "To check that you have correctly implemented the gradient correctly, you can numerically estimate the gradient of the loss function and compare the numeric estimate to the gradient that you computed. We have provided code that does this for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FgUKKiqihAU",
        "outputId": "ace38330-8acd-4470-f5d5-5452fea730ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numerical: 29.191349 analytic: 29.191349, relative error: 6.357104e-12\n",
            "numerical: 10.209957 analytic: 10.209957, relative error: 2.832170e-11\n",
            "numerical: -1.215051 analytic: -1.215051, relative error: 2.688559e-10\n",
            "numerical: -5.795541 analytic: -5.795541, relative error: 5.562482e-11\n",
            "numerical: -13.091460 analytic: -13.114688, relative error: 8.863514e-04\n",
            "numerical: 29.382702 analytic: 29.382702, relative error: 7.541749e-12\n",
            "numerical: -1.755797 analytic: -1.755797, relative error: 1.755237e-10\n",
            "numerical: 32.615205 analytic: 32.615205, relative error: 3.625129e-13\n",
            "numerical: 0.260218 analytic: 0.260218, relative error: 8.769583e-10\n",
            "numerical: -22.556951 analytic: -22.556951, relative error: 5.090785e-12\n",
            "numerical: 1.625470 analytic: 1.658575, relative error: 1.008040e-02\n",
            "numerical: -9.152016 analytic: -9.152016, relative error: 2.950529e-12\n",
            "numerical: -29.370870 analytic: -29.370870, relative error: 9.943131e-12\n",
            "numerical: -18.529368 analytic: -18.529368, relative error: 1.331978e-12\n",
            "numerical: 22.501145 analytic: 22.526252, relative error: 5.576056e-04\n",
            "numerical: -4.514983 analytic: -4.514983, relative error: 3.577415e-11\n",
            "numerical: 10.955022 analytic: 10.955022, relative error: 2.118579e-11\n",
            "numerical: 8.175088 analytic: 8.175088, relative error: 1.455891e-11\n",
            "numerical: -18.378410 analytic: -18.378410, relative error: 2.265969e-12\n",
            "numerical: 2.513381 analytic: 2.513381, relative error: 3.817457e-11\n"
          ]
        }
      ],
      "source": [
        "# Once you've implemented the gradient, recompute it with the code below\n",
        "# and gradient check it with the function we provided for you\n",
        "\n",
        "# Compute the loss and its gradient at W.\n",
        "loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# Numerically compute the gradient along several randomly chosen dimensions, and\n",
        "# compare them with your analytically computed gradient. The numbers should match\n",
        "# almost exactly along all dimensions.\n",
        "from gradient_check import grad_check_sparse\n",
        "f = lambda w: svm_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad)\n",
        "\n",
        "# do the gradient check once again with regularization turned on\n",
        "# you didn't forget the regularization gradient did you?\n",
        "loss, grad = svm_loss_naive(W, X_dev, y_dev, 5e1)\n",
        "f = lambda w: svm_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7XMhY5kihAV"
      },
      "source": [
        "Complete the implementation of svm_loss_vectorized, and compute the gradient of the loss function in a vectorized way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "sUasAsyVihAV"
      },
      "outputs": [],
      "source": [
        "def svm_loss_vectorized(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Structured SVM loss function, vectorized implementation.\n",
        "\n",
        "    Inputs and outputs are the same as svm_loss_naive.\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(W.shape)\n",
        "\n",
        "    num_train = X.shape[0]\n",
        "    scores = X.dot(W)\n",
        "    correct_class_scores = scores[np.arange(num_train), y]\n",
        "    margins = np.maximum(0, scores - correct_class_scores[:, np.newaxis] + 1)\n",
        "    margins[np.arange(num_train), y] = 0\n",
        "    loss = np.sum(margins) / num_train\n",
        "    loss += reg * np.sum(W * W)\n",
        "\n",
        "    binary = margins > 0\n",
        "    binary = binary.astype(int)\n",
        "    incorrect_counts = np.sum(binary, axis=1)\n",
        "    binary[np.arange(num_train), y] = -incorrect_counts\n",
        "    dW = X.T.dot(binary) / num_train\n",
        "    dW += 2 * reg * W\n",
        "\n",
        "    return loss, dW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vectorized_time_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35215a9a-76c2-41d0-f9cd-b1e0621e4405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive loss and gradient: computed in 0.110247s\n",
            "Vectorized loss and gradient: computed in 0.012631s\n",
            "difference: 0.000000\n"
          ]
        }
      ],
      "source": [
        "# The naive implementation and the vectorized implementation should match, but\n",
        "# the vectorized version should still be much faster.\n",
        "tic = time.time()\n",
        "_, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
        "\n",
        "tic = time.time()\n",
        "_, grad_vectorized = svm_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('Vectorized loss and gradient: computed in %fs' % (toc - tic))\n",
        "\n",
        "# The loss is a single number, so it is easy to compare the values computed\n",
        "# by the two implementations. The gradient on the other hand is a matrix, so\n",
        "# we use the Frobenius norm to compare them.\n",
        "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
        "print('difference: %f' % difference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU9jCg-gihAV"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1S904VtihAV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0hTFr54ihAV"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "new_virtual_environment",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}